---
title: "01: Geospatial Analytics for Public Good"
author: "Yiqiong PAN"
date: "Sep 10, 2025"
date-modified: "last-modified"
execute:
  echo: true #display the code
  eval: true #run the code
  message: false
  warning: false
  freeze: true # not render if nothing edited
editor: visual
format: 
  html:
    number-sections: true
    number-depth: 4
---

## Overview

Motivation

**Spatial Point Pattern Analysis** (SPPA) refers to the study of how points are arranged or distributed across a given surface. There points may represent:

-   **Events**, such as crimes, road accidents, or disease occurrences, or

-   **Service and facility locations**, including shops (e.g., cafes, supermarkets) and community facilities like childcare or aged care centres.

**First-order** **Spatial Point Pattern Analysis** (1-st SPPA) examines the *intensity* or *density* of points across a study area. It identifies spatial trends in point distribution without considering interaction between points helping to answer questions like:

-   Where are points most concentrated?

-   Is density uniform or variable?

-   How dispersed is the pattern?

Unlike first-order analysis, which looks only at overall density independently, second-order methods reveal relationships and influences between centres (points) based on distance.

A **spatio-temporal point process** is a random set of points, each showing when and where an event happens. Examples include disease cases, animal sightings, or natural disasters such as fires and earthquakes.

As more data is collected with both time and location, analysing spatio-temporal patterns has become more important. In recent years, several R packages have been developed for this purpose.

This exercise demonstrates how to apply the process through a case study of **New Business** in Singapore, from 1 January to 30 June 2025.

Since the period covers all of 2024 and first half of 2025, we split the data into two groups and compare them: Baseline: 2024 (Jan–Dec) Comparison: 2025 (Jan–Jun) This let us check whether some SSIC groups are growing faster in 2025, or whether patterns are stable.

## objective

The key questions we aim to address are:

**1. Are new business locations in Singapore independent in space and time?**

**2. If not, in which areas and during which periods do the forest fires tend to cluster?**

## The Data

For this exercise, the two datasets used are as follows:

1.  Master Plan 2019 Subzone Boundary (No Sea) (KML) contains xxx, downloaded from [data.gov.sg](http://data.gov.sg).

2.  **Corporate Entities dataset** contains corporate entitites regisretarion date since... Aug 2025, among with we extracted from duration and study buness based on SSIC 2020. THe orgindal data is downed from data.gov.sg

## Installing and Loading the R Packages

A total of **six** R packages will be used in this exercise.

| Package | Description |
|----|----|
| [sf](https://r-spatial.github.io/sf/) | Simple Features, a new R package which handles importing, managing, and processing vector-based geospatial data. |
| [raster](https://rspatial.org/raster/) | Tools for reading, writing, and analyzing raster (gridded) spatial data in R |
| [spatstat](https://spatstat.org/) | Provides useful functions for SPPA, including kcross, Lcross etc. |
| [sparr](https://tilmandavies.github.io/sparr/index.html) | Functions for fixed/adaptive kernel density estimation and relative risk mapping via density ratios; also supports fixed-bandwidth space-time density and risk estimation with inference. |
| [tmap](https://r-tmap.github.io/tmap/) | Creates cartographic quality static or interactive choropleth maps. |
| [tidyverse](https://www.tidyverse.org/) | A collection of R packages for data import, cleaning, transformation, and visualization (e.g., readr, dplyr, tidyr, ggplot2). |
| [terra](https://rspatial.github.io/terra/index.html) | Modern package for raster/vector spatial data and will be used to convert spastat outputs into terra format. |

| stopp for 2rd order ST SPPA \|

| stpp \|

| magick for animation \|

| fs for renaming \|

: {tbl-colwidths="\[15,85\]"}

After installation via `install.packages()`, we load them into R environment using the code below.

```{r}
pacman:: p_load(sf, raster, spatstat, sparr, tmap, tidyverse, terra, stopp, stpp, magick, fs,vroom, readxl,rvest, onemapsgapi,tidygeocoder)
```

## Importing and Preparing Study Area

::: panel-tabset
### Importing Study Area

The following code chunk shows the steps to first import the `Master plan 2019 Subzone Boundary (No Sea)` data using **st_read()** function from sf package, extract the required 4 columns from the `Description` field, filter out the nearby islands, and finally save the file as `mpsz_cl` for further analysis.

```{r}

mpsz_sf <- st_read("data/geospatial/rawdata/MasterPlan2019SubzoneBoundaryNoSeaKML.kml") %>%
  st_zm(drop = TRUE, what = "ZM") %>%
  st_transform(crs = 3414)

summary(mpsz_sf)

tm_shape(mpsz_sf) +
  tm_polygons() 
```

```{r}

extract_kml_field <- function(html_text, field_name) {
  if (is.na(html_text) || html_text == "") return(NA_character_)
  
  page <- read_html(html_text)
  rows <- page %>% html_elements("tr")
  
  value <- rows %>%
    keep(~ html_text2(html_element(.x, "th")) == field_name) %>%
    html_element("td") %>%
    html_text2()
  
  if (length(value) == 0) NA_character_ else value
}
```

```{r}
# map_chr of purr (tidyverse) applies a function to each element of a list/vector and returns a character vector.
mpsz_sf <- mpsz_sf %>%
  mutate(
    REGION_N = map_chr(Description, extract_kml_field, "REGION_N"),
    PLN_AREA_N = map_chr(Description, extract_kml_field, "PLN_AREA_N"),
    SUBZONE_N = map_chr(Description, extract_kml_field, "SUBZONE_N"),
    SUBZONE_C = map_chr(Description, extract_kml_field, "SUBZONE_C")
  ) %>%
  select(-Name, -Description) %>%
  relocate(geometry, .after = last_col())
```

```{r}

mpsz_cl <- mpsz_sf %>%
  filter(SUBZONE_N != "SOUTHERN GROUP",
         PLN_AREA_N != "WESTERN ISLANDS",
         PLN_AREA_N != "NORTH-EASTERN ISLANDS")

write_rds(mpsz_cl,
          "data/geospatial/mpsz_cl.rds")
```

```{r}
mpsz_cl <- read_rds("data/geospatial/mpsz_cl.rds")
summary(mpsz_cl)

tm_shape(mpsz_cl) +
  tm_polygons() 
```

### Business registry

we realise that \`preimiarly ssic code has missing leading 0 probably due to transmission and registartion date date type is ymd.

Inside map_dfr(), vroom(.x) → reads each CSV filter(between(...)) → keeps only rows in the date window. map_dfr() → row-binds all filtered CSVs into one tibble (files_fl).

```{r}
dir <- "data/aspatial/rawdata" 
files <- dir_ls(dir, glob = "*.csv") #ls: list command, glob wild card
#one_file <- vroom(files[1]) #check one file 
#spec(one_file)
files_fl <- files %>%
  map_dfr(~ vroom(.x, show_col_types = FALSE) %>% #return data frames
          filter(between(registration_incorporation_date,
                        as.Date("2024-01-01"),
                        as.Date("2025-06-30"))))
```

quick review on the column types

```{r}
files_fl
spec(files_fl)
```

sanity check

```{r}
any(files_fl$registration_incorporation_date < ymd("2024-01-01") |files_fl$registration_incorporation_date > ymd("2025-06-30"))
any(duplicated(files_fl))
```

we realise that \`preimiarly ssic code has missing leading 0 probably due to nuemvic type. so we pad the 0 to the code and create new column where 2d is extracted per row.

```{r}
files_ssic <-files_fl%>%
  mutate(`ssic_5d` = str_pad(.[[22]], width = 5, side = "left", pad = "0")) %>%
  mutate(`ssic_2d` = str_sub(`ssic_5d`,1, 2))
```

it is time to rerragnet the columsn since there were 55 columsns after the ssic_2d, we select the following columns. uen, name, reg date, type, post code, and ssic 5d, ssic 2d

```{r}
files_sl <- files_ssic %>%
  select(c(1,3:4,6,8:9,17,54,55))
```

load the colasifiaction file, and skip top 4 columns to locate the correct the collumn. select the 2d only and its depscrition.

```{r}
ssic_lookup <- read_excel("data/aspatial/ssic2020-classification-structure.xlsx", skip = 4)%>%
  select(ssic_code = `SSIC 2020`, ssic_title = `SSIC 2020 Title`) %>%
  filter(!is.na(ssic_code)) %>%
  filter(str_length(ssic_code) == 2)
```

left join with main data.

```{r}
files_sl_jn <- files_sl%>%
  left_join(ssic_lookup, by = c("ssic_2d" = "ssic_code"))
```

check the number of registeration per business group.

```{r}
files_sl_ct<- files_sl_jn %>%
  group_by(`ssic_2d`,`ssic_title`) %>% 
  summarise(n = n(), .groups = "drop") %>%
  arrange(desc(n))


top_10 <-head(files_sl_ct, 10)
top_10
```

plot a bar chart for the top 10 busienss type

```{r, fig.width=14}
top_10 %>%
  mutate(ssic_type = paste(ssic_2d,str_sub(ssic_title, 1,25), sep=":")) %>%
  ggplot(aes(x = reorder(ssic_2d, n), y = n, fill = ssic_type)) +
  geom_col() +
  #coord_filp() +
  labs(title =" Top 10 Business Registeration (1/1/2024- 30/6/2025)",
       x = "SSIC 2 Digits",
       y = "Number of Registeration",
       fill = "Business Type")+
  theme(plot.title = element_text(size = 16, face = "bold"))
```

```{r}
write.csv(files_sl_jn,"data/aspatial/files_sl_jn.csv")
```

```{r}
files_sl_jn <-vroom("data/aspatial/files_sl_jn.csv")
glimpse(files_sl_jn)
```

Narrow down to top 3–5 SSICs for deep-dive analysis SPPA/ST-SPPA
:::

## Geospatial Data Wrangling

first tidy up the postal code, some are missing lead "0"

```{r}
files_geo<-files_sl_jn%>%
  mutate(`address` = str_pad(postal_code, width = 6, side = "left", pad = "0"))

glimpse(files_geo)
```

```{r}
get_geocode_by_ssic <- function(df, ssic_code) {
  # 1) subset this SSIC
  sub <- df %>% filter(ssic_2d == ssic_code)

  # 2) unique, valid postcodes
  pc <- tibble(address = unique(sub$address)) 

  # 3) geocode (NOTE: column name passed as a STRING)
  geo <- geocode_onemap(pc, "address", return_geom = TRUE)

  # 4) join back
  sub %>% left_join(geo, by = "address") %>%
    st_as_sf(coords = c("LONGITUDE", "LATITUDE"), crs = 4326, remove = FALSE) %>%
    st_transform(crs=3414)
  
}


fs_geo  <- get_geocode_by_ssic(files_geo, "64")  #FINANCIAL SERVICES
glimspe()
#wst_geo <- get_geocode_by_ssic(files_geo, "46")  # WHOLESALE TRADE
#mcs_geo <- get_geocode_by_ssic(files_geo, "70") # MANAGEMENT CONSULTANCY ACTIVITIES
#rt_geo  <- get_geocode_by_ssic(files_geo, "47") #RETAIL TRADE
#cp_geo  <- get_geocode_by_ssic(files_geo, "62") #	COMPUTER PROGRAMMING
```

### Converting sf Point Data Frames to PPP

Here we use `as.ppp()` of spatstat package to covert the point data `bsns_sf` to ppp file, confirm the change using `class()` and have a quick overview of the data statistics via `summary()`.

### Creating Owin Object

Similarly, the owin object can be created using the function as.owin() for polygon data. After the conversion, the class() and plot() functions can be used to verify that the object is of the correct class and that the data retains its original shape.

```{r}
sg_owin <-as.owin(mpsz_cl)
```

```{r}
class(sg_owin)
```

```{r}
plot(sg_owin)
```

### Combining Point Events object and Owin Object

## Compute spatial and spatio-temporal KDE of the selected business types.

## second order spatial and spatio-temporal analysis on the selected business types.

## Describe and discuss the analysis results obtained from first and second orders of SPPA & ST SPPA.

## short report of not more than 150 words describing the spatio-temporal patterns revealed by the geovisualisation and statistical graphics.

## discuss how these findings can be used to support urban land use planning and management. (Not more than 500 words)

Check local policy during 2024–2025:

e.g., Singapore introduced incentives for fintech or retail startups?

New sustainability/green finance schemes?

Post-COVID recovery grants?

Why: If a sudden spike in registrations in certain SSIC groups coincides with a government initiative, it strengthens your explanation beyond “the data says so”.
